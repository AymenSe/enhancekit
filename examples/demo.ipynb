{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhancekit demo\n",
    "\n",
    "This notebook demonstrates the basic workflow for using `enhancekit` to inspect the\n",
    "available models and run an enhancement pass over one or more distorted images. Place\n",
    "any test inputs inside `examples/images/` before running the cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc5b7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFilter\n",
    "from IPython.display import display\n",
    "\n",
    "from enhancekit import list_models, load_model\n",
    "\n",
    "# Ensure the examples folder exists and point to the image directory\n",
    "EXAMPLES_DIR = Path(\"examples\")\n",
    "IMAGES_DIR = EXAMPLES_DIR / \"images\"\n",
    "IMAGES_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701ba574",
   "metadata": {},
   "source": [
    "## Explore the registry\n",
    "\n",
    "Inspect the registered models to decide which variant you want to try. The built-in\n",
    "examples include lightweight identity models that are useful for verifying the\n",
    "pipeline end-to-end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e351954a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the registered model identifiers\n",
    "list_models()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28178b7d",
   "metadata": {},
   "source": [
    "## Prepare an example image\n",
    "\n",
    "If you already placed a distorted image inside `examples/images`, the notebook will\n",
    "pick the first one and create a resized copy to keep the Uformer demo lightweight.\n",
    "Otherwise, it will synthesize a small noisy gradient so you can run the demo\n",
    "without additional assets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ca9e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_distorted_image() -> Path:\n",
    "    existing: Optional[Path] = next(IMAGES_DIR.glob(\"*.png\"), None)\n",
    "    if existing:\n",
    "        return existing\n",
    "\n",
    "    # Build a simple synthetic test image when no sample is provided\n",
    "    base = np.zeros((256, 256, 3), dtype=np.float32)\n",
    "    xs = np.linspace(0, 1, base.shape[1], dtype=np.float32)\n",
    "    ys = np.linspace(0, 1, base.shape[0], dtype=np.float32)\n",
    "    base[..., 0] = xs  # horizontal gradient\n",
    "    base[..., 1] = ys[:, None]  # vertical gradient\n",
    "    base[..., 2] = 0.2\n",
    "\n",
    "    noise = np.random.normal(scale=0.05, size=base.shape).astype(np.float32)\n",
    "    noisy = np.clip(base + noise, 0.0, 1.0)\n",
    "\n",
    "    image = Image.fromarray((noisy * 255).astype(\"uint8\")).filter(ImageFilter.GaussianBlur(radius=1.5))\n",
    "    out_path = IMAGES_DIR / \"synthetic_distorted.png\"\n",
    "    image.save(out_path)\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def resize_image_for_demo(path: Path, edge: int = 256) -> Path:\n",
    "    \"\"\"Create a resized copy for lightweight CPU inference.\"\"\"\n",
    "\n",
    "    image = Image.open(path)\n",
    "    if image.size == (edge, edge):\n",
    "        return path\n",
    "\n",
    "    resized = image.resize((edge, edge), resample=Image.LANCZOS)\n",
    "    resized_path = path.with_name(f\"{path.stem}_resized.png\")\n",
    "    resized.save(resized_path)\n",
    "    return resized_path\n",
    "\n",
    "\n",
    "example_path = load_distorted_image()\n",
    "print(f\"Using example image: {example_path}\")\n",
    "display(Image.open(example_path))\n",
    "\n",
    "resized_path = resize_image_for_demo(example_path, edge=256)\n",
    "if resized_path != example_path:\n",
    "    print(f\"Resized demo image saved to: {resized_path}\")\n",
    "else:\n",
    "    print(\"Image already at the demo-friendly size.\")\n",
    "example_path = resized_path\n",
    "display(Image.open(example_path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a740bb",
   "metadata": {},
   "source": [
    "## Run enhancement\n",
    "\n",
    "Load the registered Uformer model and run `enhance_image` to generate the enhanced\n",
    "output. A resized 256x256 copy keeps the forward pass quick on CPU. You can adjust\n",
    "`device` to use a GPU if one is available and pick any registered model name from\n",
    "the list above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e0fcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\n",
    "    \"uformer\",\n",
    "    pretrained=False,\n",
    "    device=\"cpu\",\n",
    "    freeze=True,\n",
    "    img_size=256,\n",
    ")\n",
    "\n",
    "# Enhance a single image\n",
    "result = model.enhance_image(example_path)\n",
    "\n",
    "enhanced_path = IMAGES_DIR / \"enhanced_preview.png\"\n",
    "result.save(enhanced_path)\n",
    "\n",
    "print(f\"Enhanced image saved to: {enhanced_path}\")\n",
    "display(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d94bbe",
   "metadata": {},
   "source": [
    "## Quick Uformer sanity check\n",
    "\n",
    "Initialize a lightweight Uformer instance using the registry identity so the\n",
    "configuration stays in sync with the rest of the codebase. Overriding the\n",
    "`img_size`, `embed_dim`, and other hyperparameters keeps the forward pass small\n",
    "for CPU execution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766dba82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "lightweight_uformer = load_model(\n",
    "    \"uformer\",\n",
    "    pretrained=False,\n",
    "    device=\"cpu\",\n",
    "    freeze=True,\n",
    "    img_size=64,\n",
    "    embed_dim=32,\n",
    "    depths=[2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
    "    num_heads=[1, 2, 4, 8, 16, 16, 8, 4, 2],\n",
    "    win_size=8,\n",
    "    token_projection=\"linear\",\n",
    "    token_mlp=\"leff\",\n",
    "    modulator=False,\n",
    "    shift_flag=True,\n",
    ")\n",
    "\n",
    "backbone = getattr(lightweight_uformer, \"backbone\", lightweight_uformer)\n",
    "torch.manual_seed(0)\n",
    "fake_input = torch.rand(1, 3, 64, 64)\n",
    "backbone.eval()\n",
    "with torch.no_grad():\n",
    "    fake_output = backbone(fake_input)\n",
    "\n",
    "print(f\"Fake input shape: {tuple(fake_input.shape)} -> output shape: {tuple(fake_output.shape)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5ba41d",
   "metadata": {},
   "source": [
    "## Batch and folder utilities\n",
    "\n",
    "Enhance an entire folder by reusing the same model. Outputs are written to a sibling\n",
    "`examples/outputs` directory so you can compare them with the originals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaac5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = EXAMPLES_DIR / \"outputs\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Run over every image in the folder\n",
    "output_paths = model.enhance_folder(IMAGES_DIR, output_folder=OUTPUT_DIR)\n",
    "\n",
    "print(f\"Wrote {len(output_paths)} enhanced files to {OUTPUT_DIR}\")\n",
    "for path in output_paths:\n",
    "    display(Image.open(path))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}