{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Enhancekit demo\n\nThis notebook demonstrates the basic workflow for using `enhancekit` to inspect the\navailable models and run an enhancement pass over one or more distorted images. Place\nany test inputs inside `examples/images/` before running the cells.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pathlib import Path\nfrom typing import Optional\n\nimport numpy as np\nfrom PIL import Image, ImageFilter\nfrom IPython.display import display\n\nfrom enhancekit import list_models, load_model\n\n# Ensure the examples folder exists and point to the image directory\nEXAMPLES_DIR = Path(\"examples\")\nIMAGES_DIR = EXAMPLES_DIR / \"images\"\nIMAGES_DIR.mkdir(parents=True, exist_ok=True)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Explore the registry\n\nInspect the registered models to decide which variant you want to try. The built-in\nexamples include lightweight identity models that are useful for verifying the\npipeline end-to-end.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# List the registered model identifiers\nlist_models()\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Prepare an example image\n\nIf you already placed a distorted image inside `examples/images`, the notebook will\npick the first one. Otherwise, it will synthesize a small noisy gradient so you can\nrun the demo without additional assets.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def load_distorted_image() -> Path:\n    existing: Optional[Path] = next(IMAGES_DIR.glob(\"*.png\"), None)\n    if existing:\n        return existing\n\n    # Build a simple synthetic test image when no sample is provided\n    base = np.zeros((256, 256, 3), dtype=np.float32)\n    xs = np.linspace(0, 1, base.shape[1], dtype=np.float32)\n    ys = np.linspace(0, 1, base.shape[0], dtype=np.float32)\n    base[..., 0] = xs  # horizontal gradient\n    base[..., 1] = ys[:, None]  # vertical gradient\n    base[..., 2] = 0.2\n\n    noise = np.random.normal(scale=0.05, size=base.shape).astype(np.float32)\n    noisy = np.clip(base + noise, 0.0, 1.0)\n\n    image = Image.fromarray((noisy * 255).astype(\"uint8\")).filter(ImageFilter.GaussianBlur(radius=1.5))\n    out_path = IMAGES_DIR / \"synthetic_distorted.png\"\n    image.save(out_path)\n    return out_path\n\n\nexample_path = load_distorted_image()\nprint(f\"Using example image: {example_path}\")\ndisplay(Image.open(example_path))\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Run enhancement\n\nLoad a model from the registry and run `enhance_image` to generate the enhanced output.\nYou can adjust `device` to use a GPU if one is available and pick any registered model\nname from the list above.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "model = load_model(\"identity_gain2\", device=\"cpu\", freeze=True)\n\n# Enhance a single image\nresult = model.enhance_image(example_path)\n\nenhanced_path = IMAGES_DIR / \"enhanced_preview.png\"\nresult.save(enhanced_path)\n\nprint(f\"Enhanced image saved to: {enhanced_path}\")\ndisplay(result)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Uformer sanity check\n",
    "\n",
    "Initialize a `Uformer` instance using the original configuration values (embed_dim,\n",
    "depths, attention heads, and token settings) while keeping the spatial size small\n",
    "so the forward pass stays lightweight on CPU. This avoids downloading weights and\n",
    "just validates that the architecture executes end-to-end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from enhancekit.models.uformer import Uformer\n",
    "\n",
    "torch.manual_seed(0)\n",
    "uformer = Uformer(\n",
    "    img_size=64,\n",
    "    embed_dim=32,\n",
    "    depths=[2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
    "    num_heads=[1, 2, 4, 8, 16, 16, 8, 4, 2],\n",
    "    win_size=8,\n",
    "    token_projection=\"linear\",\n",
    "    token_mlp=\"leff\",\n",
    "    modulator=False,\n",
    "    shift_flag=True,\n",
    ")\n",
    "\n",
    "fake_input = torch.rand(1, 3, 64, 64)\n",
    "uformer.eval()\n",
    "with torch.no_grad():\n",
    "    fake_output = uformer(fake_input)\n",
    "\n",
    "print(f\"Fake input shape: {tuple(fake_input.shape)} -> output shape: {tuple(fake_output.shape)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Batch and folder utilities\n\nEnhance an entire folder by reusing the same model. Outputs are written to a sibling\n`examples/outputs` directory so you can compare them with the originals.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "OUTPUT_DIR = EXAMPLES_DIR / \"outputs\"\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\n# Run over every image in the folder\noutput_paths = model.enhance_folder(IMAGES_DIR, output_folder=OUTPUT_DIR)\n\nprint(f\"Wrote {len(output_paths)} enhanced files to {OUTPUT_DIR}\")\nfor path in output_paths:\n    display(Image.open(path))\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}